{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd90f55",
   "metadata": {},
   "source": [
    "## NLP Lecture for PPD 559\n",
    "\n",
    "In this lecture, you will be introduced to using Natural Language Processing (NLP) in urban analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e3b10",
   "metadata": {},
   "source": [
    "Objectives for this lecture:\n",
    "\n",
    "1. Understand and use common NLP python packages.\n",
    "2. Find and visualize patterns in language topics. \n",
    "3. Relate language and topics to the underlying urban landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7677d9ce",
   "metadata": {},
   "source": [
    "### What is NLP and how can you use it?\n",
    "\n",
    "NLP is ability to process text or spoken word based data with a computer in order to efficiently deal with large, potentially unruly or unstructured, data. \n",
    "\n",
    "In urban analytics, the uses of NLP are boundless! You can now handle large amounts of data coming from plans themselves, online open response questionaires, social media postings, transcripts from interviews or meetings, and more. Each of these datasets can illuminate important themes that may be difficult or time consuming to find by hand.\n",
    "\n",
    "The NLP processing chain is most often:\n",
    "1. Preprocess data to make text as uniform as possible.\n",
    "2. Decide what each \"document\" should be - whole body, paragraph, sentence, few words, etc.\n",
    "3. Turn each document into vector.\n",
    "4. Utilize various existing tools with vectorized data.\n",
    "5. Analyze results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk \n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa07527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's work with some example data from Zillow \n",
    "data = pd.read_csv('../../data/newyork_housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9aa632",
   "metadata": {},
   "source": [
    "### 1. Introduction to nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b2b60",
   "metadata": {},
   "source": [
    "One of the most powerful tools in Python for NLP is the natural langauge toolkit (nltk) (https://www.nltk.org/). It is rich with processes and easy to use. Often, this package is used for the preprocessing stage where your text data may undergo any of the following:\n",
    "\n",
    "#### - forcing lowercase and removing unwanted symbols\n",
    "Ultimately, you are working with one string composed of different symbols (letters and numbers), so creating uniformity however possible is helpful. You want the computer to recognize \"T\" and \"t\" as the same symbol. You might not also want your application to care about \"*\" or \"|\". It all depends on what you want to pick up on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate the text column\n",
    "bodytext = data['description']\n",
    "#make all letters lowercase\n",
    "bodytext = bodytext.str.lower()\n",
    "#remove non alphabetic characters \n",
    "bodytext = bodytext.apply(lambda x: re.sub(\"[^A-Za-z']+\", ' ', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the before\n",
    "print(data['description'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03590280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the after\n",
    "print(bodytext.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a65f1",
   "metadata": {},
   "source": [
    "#### - removing \"stopwords\"\n",
    "\n",
    "Stopwords are very common, usually insignificant words that you want filtered out before you do any processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6602343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at some of the \"stopwords\"\n",
    "nltk.corpus.stopwords.words('english')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8970b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove these from each document\n",
    "bodytext = bodytext.apply(lambda x: x.split(\" \"))\n",
    "no_stopwords = bodytext.apply(lambda x: sorted(set(x) - set(nltk.corpus.stopwords.words('english')), key=x.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40264876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now view our sample text without any stopwords \n",
    "print(no_stopwords.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a35ad",
   "metadata": {},
   "source": [
    "#### - stemming or lemmatizing \n",
    "\n",
    "Stemming is the process of taking a word down to its root. Lemmatizing is the process of changing a word to its base format. Either step is usually performed in order to help your model capture variations in how people might represent words. For example, if you wanted to know how often people were talking about change in a system, you would want to capture whenever people say \"change\", \"changing\", \"changes\", or \"changed\". You can see how this would happen for stemming vs lemmatizing below.\n",
    "\n",
    "| Stemming | Lemmatizing |\n",
    "| --- | --- | \n",
    "| change $\\rightarrow$ chang | change $\\rightarrow$ change | \n",
    "| changes $\\rightarrow$ chang | changes $\\rightarrow$ change | \n",
    "| changing $\\rightarrow$ chang | changing $\\rightarrow$ change | \n",
    "| changed $\\rightarrow$ chang | changed $\\rightarrow$ change | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stem each word \n",
    "#initialze Stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "#apply to each word in each document\n",
    "bodytext_stemmed = bodytext.apply(lambda x: [stemmer.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8543578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view our sample text after being stemmed\n",
    "print(bodytext_stemmed.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12929c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize each word\n",
    "#initialize Lemmatizer\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "#apply to each word in each document\n",
    "bodytext_lemm = bodytext.apply(lambda x: [wnl.lemmatize(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view our sample text after being lemmatized\n",
    "print(bodytext_lemm.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b3f05",
   "metadata": {},
   "source": [
    "#### NLTK has other powerful accessories!\n",
    "\n",
    "nltk can help identify the part of speech to isolate nouns, verbs, adjectives, etc. It can also identify groupings of words that most often occur together!\n",
    "\n",
    "The nltk POS codes are: \n",
    "\n",
    "| Code | Part of Speech || Code | Part of Speech |\n",
    "| --- | --- || --- | --- |  \n",
    "|CC:| conjunction, coordinating ||PDT:| pre-determiner |\n",
    "|CD:| numeral, cardinal ||POS:| genitive marker |\n",
    "|DT:| determiner ||PRP:| pronoun, personal |\n",
    "|EX:| existential there ||RB:| adverb |\n",
    "|IN:| preposition or conjunction, subordinating ||RP:| particle |\n",
    "|JJ:| adjective or numeral, ordinal ||TO:| \"to\" as preposition or infinitive marker |\n",
    "|JJR:| adjective, comparative ||UH:| interjection |\n",
    "|JJS:| adjective, superlative ||VB:| verb, base form |\n",
    "|LS:| list item marker ||VBD:| verb, past tense |\n",
    "|MD:| modal auxiliary ||VBG:| verb, present participle or gerund |\n",
    "|NN:| noun, common, singular or mass ||VBN:| verb, past participle |\n",
    "|NNP:| noun, proper, singular || WDT:| WH-determiner |\n",
    "|NNS:| noun, common, plural|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a50fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the part of speech and isolate adjectives, nouns, etc.\n",
    "example_sentence = bodytext.iloc[0]\n",
    "print(nltk.pos_tag(example_sentence)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at all of the adjectives for the postings\n",
    "def keep_pos(x,pos=['JJ','JJS','JJR']):\n",
    "    tagged = nltk.pos_tag(x)\n",
    "    words_to_keep = [t[0] for t in tagged if t[1] in pos]\n",
    "    return words_to_keep\n",
    "\n",
    "keep_pos(example_sentence, pos=['JJ','JJS','JJR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify words that often appear together\n",
    "number_of_words = 2\n",
    "ngrams = no_stopwords.apply(lambda x: list(nltk.ngrams(x,number_of_words)))\n",
    "count = Counter(list(chain.from_iterable(list(ngrams.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now your turn\n",
    "#Identify the most common words across the whole dataset at each stage to see how the list changes\n",
    "#With the original data, with lowercasing, with removing stopwords, with stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4edc86",
   "metadata": {},
   "source": [
    "### 2. Introduction to TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c679bb",
   "metadata": {},
   "source": [
    "Step 2 of the NLP process is determining what your \"document\" will be. This can be the whole text as one, each sentence individually, or even bi- or tri-grams of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split by sentence\n",
    "def split_by_sent(text, split_criteria=['  ','.', '!', '?','\\n']):\n",
    "    for x in split_criteria:\n",
    "        text = str(text).replace(x, '*')\n",
    "    bodylist = str(text).split('*')\n",
    "    bodylist = [w for w in bodylist if w != '']\n",
    "    return bodylist    \n",
    "    \n",
    "sentences = data['description'].str.lower().apply(lambda x: split_by_sent(x))\n",
    "sentencedf = sentences.explode()\n",
    "sentencedf = sentencedf[~sentencedf.isna()]\n",
    "print(sentences.iloc[0])\n",
    "print('\\n', sentencedf.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71192ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split by bigram\n",
    "bigrams = no_stopwords.apply(lambda x: list(nltk.ngrams(x,2)))\n",
    "bigramdf = bigrams.explode()\n",
    "print(bigrams.iloc[0])\n",
    "print('\\n', bigramdf.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a438f02",
   "metadata": {},
   "source": [
    "One method of performing step 3, turning each document into a vector, is through Term Frequency-Inverse Document Frequency (TFIDF). TF-IDF measures how important each word is to each document. \n",
    "\n",
    "Term Frequency (tf) refers to how often a word occurs in a document, ranging from 0 to 1. Inverse document frequency (idf) refers to how often a word occurs in _any_ of the documents, where closer to 0 represents more common words (think: and, the, it) and closer to 1 represents rarer words (think: quire, ulotrichous).\n",
    "\n",
    "The goal is to have a vector for each document that is 1 x n (n being the total number of words in the dataset dictionary) with values describing the tf * idf scores for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we need a vector that shows the counts of each word in each document. Most of it will be 0.\n",
    "documents = bodytext.apply(lambda x: ' '.join(x))\n",
    "count_vect = CountVectorizer()\n",
    "data_counts = count_vect.fit_transform(documents)\n",
    "#Then, we can create the tf-idf matrix\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "data_tfidf = tfidf_transformer.fit_transform(data_counts)\n",
    "#Inspect the shape of the matrix\n",
    "print(data_counts.shape)\n",
    "print(data_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc77953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now with the sentence dataframe\n",
    "#First, we need a vector that shows the counts of each word in each document. Most of it will be 0.\n",
    "count_vect_sent = CountVectorizer()\n",
    "data_counts_sentences = count_vect_sent.fit_transform(sentencedf)\n",
    "#Then, we can create the tf-idf matrix\n",
    "tfidf_transformer_sent = TfidfTransformer()\n",
    "data_tfidf_sentences = tfidf_transformer_sent.fit_transform(data_counts_sentences)\n",
    "#Inspect the shape of the matrix\n",
    "print(data_counts_sentences.shape)\n",
    "print(data_tfidf_sentences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d78217",
   "metadata": {},
   "source": [
    "### 3. Introduction to word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81483807",
   "metadata": {},
   "source": [
    "Another method of performing step 3, turning a document into a vector, is through a \"word2vec\" model, which as you might have guessed, turns words in2 vectors!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b02052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, load a model pretrained on Google News articles\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e1a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how the word \"house\" is embedded in the vector space\n",
    "vector = wv['house'] \n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can see the most similar words in the corpus\n",
    "wv.most_similar('house', topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd1e818",
   "metadata": {},
   "source": [
    "### 4. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa10e296",
   "metadata": {},
   "source": [
    "Now that we have our documents represented as a matrix (m documents x n words in dictionary OR m documents x n features in word2vec vector), we want to understand what topics are present "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eff652",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "LDA is an unsupervised topic modeling technique. We can use this technique to create clusters, or topics, that are commonly occuring across all of the documents. Then, we can understand what words describe those topics. Finally, we can trace the topics back to our documents (remember, this can be the full ad or a single sentence) and see what topics appear in each document. There can be more than one topic per document!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b734ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary \n",
    "documents = sentencedf.apply(lambda x: x.split(\" \"))\n",
    "documents = documents.apply(lambda x: sorted(set(x) - set(nltk.corpus.stopwords.words('english')), key=x.index))\n",
    "all_text = list(documents)\n",
    "all_dict = corpora.Dictionary(all_text)\n",
    "doc_term_matrix = [all_dict.doc2bow(i) for i in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7512f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose number of topics and create model\n",
    "num_topics = 12\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrix,\n",
    "                             id2word=all_dict,\n",
    "                             num_topics=num_topics,\n",
    "                             eval_every=None,\n",
    "                             passes=1,\n",
    "                             random_state=0)\n",
    "\n",
    "#save the top num_words for each topic \n",
    "num_words = 15\n",
    "print_topics = ldamodel.print_topics(num_topics=num_topics, num_words=num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb079b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in print_topics:\n",
    "    print('Topic {}'.format(topic[0]))\n",
    "    topwords = topic[1].split('\"')[1::2]\n",
    "    print(\", \".join(topwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5648f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_top_topics = []\n",
    "for i in range(len(documents)):\n",
    "    topic_probs = ldamodel[doc_term_matrix[i]]\n",
    "    max_score = 0\n",
    "    top_topic = num_topics\n",
    "    for topic, prob in topic_probs:\n",
    "        if prob > max_score:\n",
    "            max_score = prob\n",
    "            top_topic = topic\n",
    "    doc_top_topics.append(top_topic)\n",
    "\n",
    "\n",
    "sentencedf2 = pd.DataFrame({'adindex': sentencedf.index, \n",
    "                            'sentence': sentencedf.values, \n",
    "                            'top_topic': doc_top_topics, \n",
    "                            'sent_len': documents.apply(len)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate what percentage of the ad is dedicated to each topic \n",
    "import numpy as np\n",
    "percentages = np.zeros((len(data),num_topics))\n",
    "#groupby the ad and the topic of the sentence. Sum the number of words per ad per topic\n",
    "groupeddf = sentencedf2.groupby(['adindex', 'top_topic']).sent_len.sum()\n",
    "#Put into a matrix\n",
    "for idx in groupeddf.index:\n",
    "    percentages[idx] = groupeddf[idx]\n",
    "percentages = np.transpose(np.transpose(percentages)/percentages.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the percentage of the ads dedicated to each topic \n",
    "pd.DataFrame(data=percentages, columns = range(num_topics)).boxplot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7da9dc",
   "metadata": {},
   "source": [
    "#### Similar to keywords using word2vec model\n",
    "\n",
    "We can also use our word2vec model to find how similar our document is to a predetermined keyword or topic! We can do this by testing how similar all of the words within the document are to the keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set of words to compare ad words to \n",
    "testwords = ['transit'] \n",
    "\n",
    "#get similarity scores of each word in each ad to keywords in testwords list\n",
    "sims = []\n",
    "for row in no_stopwords.values:\n",
    "    tmp= [], \n",
    "    for w in row:\n",
    "        #look at the similarity score between each word and the testwords\n",
    "        try:\n",
    "            tmp.append(wv.similarity(testwords[0], w))\n",
    "        #not all words are in our corpus defined under the wv model\n",
    "        except:\n",
    "            continue\n",
    "    sims.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean of similarity scores for each keyword / document pair\n",
    "means = np.zeros((len(sims), 1))\n",
    "means[:,0] = list(map(lambda x: np.mean(x), sims))\n",
    "\n",
    "#plot distributions of keyword similarity scores\n",
    "plt.close()\n",
    "fig, ax = plt.subplots()\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "ax.patch.set_alpha(0)\n",
    "sns.distplot(means[:,0])\n",
    "plt.xlabel('Mean Similarity Score of Words in Body Text')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0.05,0.2)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now your turn\n",
    "#Look at the topics determined by our LDA method when using the whole ad \n",
    "#Or try out the similarity scores for another keyword\n",
    "#Or try to use our tf-idf vectors in another clustering method you know (k-means, dbscan, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640acf58",
   "metadata": {},
   "source": [
    "### 5. Putting it Together with Spatial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76782912",
   "metadata": {},
   "source": [
    "Once you have performed your text analysis, often you will end up with quantitative variables which can then be analyzed spatially as with any other data. \n",
    "\n",
    "You might have now integer values representing the most prominent topic for each document, the percent of the text dedicated to a word or topic, or even simply the boolean presence of a word or topic. If the documents contain some sort of spatial information (e.g., location of the Zillow ad), you can now perform your spatial analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640dbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import branca.colormap as cm\n",
    "\n",
    "#add the topic percentages to the original dataframe\n",
    "data_new = data.join(pd.DataFrame(data=percentages, columns = range(num_topics)).fillna(0)) \n",
    "data_new = data_new[~data_new.latitude.isna()]\n",
    "\n",
    "#create the map\n",
    "centerlat = (data_new['latitude'].max() + data_new['latitude'].min()) / 2\n",
    "centerlong = (data_new['longitude'].max() + data_new['longitude'].min()) / 2\n",
    "center = (centerlat, centerlong)\n",
    "colormap = cm.LinearColormap(colors=['green', 'yellow', 'red'], vmin=0, vmax=1)\n",
    "map_nyc = folium.Map(location=center, zoom_start=10, tiles='Stamen Toner')\n",
    "\n",
    "#topic_data1\n",
    "topic_number1 = 0\n",
    "for i in range(len(data_new)):\n",
    "    folium.Circle(\n",
    "        location=[data_new.iloc[i]['latitude'], data_new.iloc[i]['longitude']],\n",
    "        radius=10,\n",
    "        fill=True,\n",
    "        color=colormap(data_new.iloc[i][topic_number1]),\n",
    "        fill_opacity=0.2\n",
    "    ).add_to(map_nyc)\n",
    "\n",
    "# the following line adds the scale directly to our map\n",
    "map_nyc.add_child(colormap)\n",
    "\n",
    "map_nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc802a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_nyc2 = folium.Map(location=center, zoom_start=10, tiles='Stamen Toner')\n",
    "\n",
    "#topic_data2\n",
    "topic_number2 = 10\n",
    "for i in range(len(data_new)):\n",
    "    folium.Circle(\n",
    "        location=[data_new.iloc[i]['latitude'], data_new.iloc[i]['longitude']],\n",
    "        radius=10,\n",
    "        fill=True,\n",
    "        color=colormap(data_new.iloc[i][topic_number2]),\n",
    "        fill_opacity=0.2\n",
    "    ).add_to(map_nyc2)\n",
    "\n",
    "# the following line adds the scale directly to our map\n",
    "map_nyc2.add_child(colormap)\n",
    "\n",
    "map_nyc2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4af73",
   "metadata": {},
   "source": [
    "### 6. Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c5464",
   "metadata": {},
   "source": [
    "Work through the above examples to identify a pattern of your choosing. \n",
    "Separate the data initially and see how your topics vary. \n",
    "\n",
    "For example, what LDA topics emerge when you separate on listing price? on number of bedrooms? on square footage?\n",
    "\n",
    "What keywords can you search on for similarity with the word2vec model? How do the distributions change across the above separations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d50150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c53dec24",
   "metadata": {},
   "source": [
    "### 7. BONUS - Working with Different Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect the language(s) of your text along with a confidence score\n",
    "from googletrans import Translator\n",
    "def detect_lang(text):\n",
    "    translator = Translator()\n",
    "    detection=translator.detect(text)\n",
    "    return  detection.confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_lang(bodytext.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8397fe-6b6d-44e3-a2ac-c986f0f98f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
