{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Models\n",
    "\n",
    "Overview of today's topics:\n",
    "\n",
    "  - quick refresher\n",
    "  - spatial fixed effects\n",
    "  - spatial regimes\n",
    "  - spatial lag\n",
    "  - spatial error\n",
    "  - geographically-weighted regression\n",
    "  \n",
    "## 1. Quick refresher\n",
    "\n",
    "### 1.1. Theory and models\n",
    "\n",
    "**Spatial models** are models that include geographic information to account for spatial relationships and processes. They can take on many different forms:\n",
    "\n",
    "  - Spatially-explicit regression models (with [PySAL](https://pysal.org))\n",
    "  - Agent-based modeling and/or cellular automata (with [Mesa](https://mesa.readthedocs.io/))\n",
    "  - Bayesian spatial models using Markov chain Monte Carlo methods (with [PyMC3](https://docs.pymc.io/))\n",
    "\n",
    "We will focus on spatially-explicit regression models in this module. Spatially-explicit regression models are a type of **statistical model**: sets of assumptions plus mathematical relationships between variables, producing a formal representation of some theory. We are essentially trying to explain the process underlying the generation of our observed data. **Spatial inference** introduces explicit spatial relationships into the statistical modeling framework, as both theory-driven (e.g., spatial spillovers) and data-driven (e.g., MAUP) issues could otherwise violate modeling assumptions.\n",
    "\n",
    "### 1.2. Statistical inference refresher\n",
    "\n",
    "**Statistical inference** is the process of using a sample to *infer* the characteristics of an underlying population (from which this sample was drawn) through estimation and hypothesis testing. What is the probability distribution (the probabilities of occurrence of different possible outcome values of our response variable)? Contrast this with descriptive statistics, which focus simply on describing the characteristics of the sample itself.\n",
    "\n",
    "Common goals of inferential statistics include:\n",
    "\n",
    "  - parameter estimation and confidence intervals\n",
    "  - hypothesis rejection\n",
    "  - prediction and explanation\n",
    "  - model selection\n",
    "\n",
    "Schools of statistical inference:\n",
    "\n",
    "  - frequentist\n",
    "    - frequentists think of probability as proportion of time some outcome occurs (relative frequency)\n",
    "    - given lots of repeated trials, how likely is the observed outcome?\n",
    "    - concepts: statistical hypothesis testing, *p*-values, confidence intervals\n",
    "  - bayesian\n",
    "    - bayesians think of probability as amount of certainty observer has about an outcome occurring (subjective probability)\n",
    "    - probability as a measure of how much info the observer has about the real world, updated as info changes\n",
    "    - concepts: prior probability, likelihood, bayes' rule, posterior probability\n",
    "    \n",
    "### 1.3. Regression refresher\n",
    "\n",
    "This course presumes you're already comfortable with multiple regression and OLS, as a prerequisite.\n",
    "\n",
    "Regression assumptions:\n",
    "\n",
    "  - an additive, linear relationship between response and predictors\n",
    "  - uncorrelated predictors\n",
    "  - uncorrelated, homoskedastic, normally-distributed errors\n",
    "  \n",
    "Regression topics:\n",
    "\n",
    "  - specification: choosing variables to include and the functional form\n",
    "  - transformation: pre-processing to improve the linear fit (log, power, etc)\n",
    "  - estimation: using an algorithm (such as OLS, WLS, MLE, etc) to estimate (aka, fit or train) your model's parameters\n",
    "  - validation and diagnostics: model's goodness of fit ($R^2$), parameters' statistical significance ($t$-test and $p$-values), check errors and assumptions (residual plot, Q-Q plot, etc), outlier influence (leverage), robustness checks (alternative specifications)\n",
    "  - resampling: cross-validation (out-of-sample test with training/testing subsets) and bootstrapping (random subsampling to generate estimates' distribution)\n",
    "  - model selection and regularization: bias-variance tradeoff (overfitting and underfitting), lasso (L1 regularization), ridge (L2 regularization), hyperparameters\n",
    "  \n",
    "## 2. Setup and data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pysal as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CA tracts\n",
    "tracts_ca = gpd.read_file('../../data/tl_2017_06_tract/').set_index('GEOID')\n",
    "\n",
    "# keep LA, ventura, orange counties only (and drop offshore island tracts)\n",
    "to_drop = ['06037599100', '06037599000', '06111980000', '06111990100', '06111003612']\n",
    "tracts_ca = tracts_ca[tracts_ca['COUNTYFP'].isin(['037', '059', '111'])].drop(index=to_drop)\n",
    "\n",
    "# project tracts\n",
    "crs = '+proj=utm +zone=11 +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "tracts_ca = tracts_ca.to_crs(crs)\n",
    "tracts_ca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CA tract-level census variables\n",
    "df_census = pd.read_csv('../../data/census_tracts_data_ca.csv', dtype={'GEOID10':str}).set_index('GEOID10')\n",
    "df_census.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tract geometries with census variables and create med home value 1000s\n",
    "tracts = tracts_ca.merge(df_census, left_index=True, right_index=True, how='left')\n",
    "tracts['med_home_value_k'] = tracts['med_home_value'] / 1000\n",
    "tracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which variables to use as predictors\n",
    "predictors = ['pct_white', 'pct_built_before_1940', 'med_rooms_per_home', 'pct_bachelors_degree']\n",
    "\n",
    "# choose a response variable and drop any rows in which it is null\n",
    "response = 'med_home_value_k'\n",
    "tracts = tracts.dropna(subset=[response])\n",
    "tracts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get our data into the right format for estimating our model on them:\n",
    "  - the **design matrix** is a $n×k$ matrix of $n$ non-null observations on $k$ predictor variables.\n",
    "  - the **response matrix** is a $n×1$ matrix of $n$ non-null observations on the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix of predictors (drop nulls) and response matrix\n",
    "X = tracts[predictors].dropna()\n",
    "Y = tracts.loc[X.index][[response]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate linear regression model with OLS\n",
    "ols = ps.model.spreg.OLS(y=Y.values,\n",
    "                         x=X.values,\n",
    "                         name_x=X.columns.tolist(),\n",
    "                         name_y=response,\n",
    "                         name_ds='tracts')\n",
    "print(ols.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of spatially explicit models:\n",
    "\n",
    "  - **Spatial heterogeneity**: account for systematic differences across space without explicitly modeling interdependency\n",
    "    - *spatial fixed effects*: intercept varies for each spatial group\n",
    "    - *spatial regimes*: intercept and coefficients vary for each spatial group\n",
    "    - *geographically weighted regression*: model local relationships that vary across study area\n",
    "  - **Spatial dependence**: model interdependencies between observations through space\n",
    "    - *spatial lag model*: spatially-lagged endogenous variable added as predictor (because of endogeneity, cannot use OLS to estimate)\n",
    "    - *spatial error model*: spatial effects in error term\n",
    "    - *spatial combo model*: both lag and error\n",
    "    \n",
    "## 3. Spatial fixed effects\n",
    "\n",
    "Intercept varies for each each spatial group. Use dummy variables to represent the groups (counties) into which our observations (tracts) are nested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dummy variable for each county, with 1 if tract is in this county and 0 if not\n",
    "for county in tracts['COUNTYFP'].unique():\n",
    "    new_col = f'dummy_county_{county}'\n",
    "    tracts[new_col] = (tracts['COUNTYFP'] == county).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove one dummy from dummies to prevent perfect collinearity\n",
    "# ie, a subset of predictors sums to 1 (which full set of dummies will do)\n",
    "county_dummies = [f'dummy_county_{county}' for county in tracts['COUNTYFP'].unique()]\n",
    "county_dummies = county_dummies[:-1]\n",
    "county_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix of predictors (drop nulls) and response matrix\n",
    "X = tracts[predictors + county_dummies].dropna()\n",
    "Y = tracts.loc[X.index][[response]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate linear regression model with spatial fixed effects\n",
    "ols = ps.model.spreg.OLS(y=Y.values,\n",
    "                         x=X.values,\n",
    "                         name_x=X.columns.tolist(),\n",
    "                         name_y=response,\n",
    "                         name_ds='tracts')\n",
    "print(ols.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial regimes\n",
    "\n",
    "Intercept and coefficients vary for each spatial group (aka, regime). Here, the regimes are our 3 counties and we use OLS for estimation, but you can also combine spatial regimes with spatial autoregressive models (the latter is introduced later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix of predictors (drop nulls), response matrix, and regimes vector\n",
    "X = tracts[predictors].dropna()\n",
    "Y = tracts.loc[X.index][[response]]\n",
    "regimes = tracts.loc[X.index]['COUNTYFP']\n",
    "regimes.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate spatial regimes model with OLS\n",
    "olsr = ps.model.spreg.OLS_Regimes(y=Y.values,\n",
    "                                  x=X.values,\n",
    "                                  regimes=regimes.values,\n",
    "                                  name_regimes='county',\n",
    "                                  name_x=X.columns.tolist(),\n",
    "                                  name_y=response,\n",
    "                                  name_ds='tracts')\n",
    "print(olsr.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Geographically weighted regression\n",
    "\n",
    "GWR allows us to further investigate how model parameters and performance may vary across the study area. It estimates the regression model for each observation's local neighborhood, then combines these estimates into a global model for the study area. A user-defined *bandwidth* determines how these local estimates are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to generate colormaps for GWR plots\n",
    "def get_cmap(values, cmap_name='coolwarm', n=256):\n",
    "    import numpy as np\n",
    "    from matplotlib.colors import LinearSegmentedColormap as lsc\n",
    "    name = f'{cmap_name}_new'\n",
    "    cmap = plt.cm.get_cmap(cmap_name)\n",
    "    vmin = values.min()\n",
    "    vmax = values.max()\n",
    "\n",
    "    if vmax < 0:\n",
    "        # if all values are negative, use the negative half of the colormap\n",
    "        return lsc.from_list(name, cmap(np.linspace(0, 0.5, n)))\n",
    "    elif vmin > 0:\n",
    "        # if all values are positive use the positive half of the colormap\n",
    "        return lsc.from_list(name, cmap(np.linspace(0.5, 1, n)))\n",
    "    else:\n",
    "        # otherwise there are positive and negative values so use zero as midpoint\n",
    "        # and truncate the colormap such that if the original spans ± the greatest\n",
    "        # absolute value, we only use colors from it spanning vmin to vmax\n",
    "        abs_max = max(values.abs())\n",
    "        start = (vmin + abs_max) / (abs_max * 2)\n",
    "        stop = (vmax + abs_max) / (abs_max * 2)\n",
    "        return lsc.from_list(name, cmap(np.linspace(start, stop, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# select a bandwidth value for our GWR model, given the data\n",
    "centroids = tracts.loc[X.index].centroid\n",
    "coords = list(zip(centroids.x, centroids.y))\n",
    "bw = ps.model.mgwr.sel_bw.Sel_BW(coords, Y.values, X.values).search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the selected bandwidth value?\n",
    "bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# estimate the GWR model parameters\n",
    "model = ps.model.mgwr.gwr.GWR(coords=coords,\n",
    "                              y=Y.values,\n",
    "                              X=X.values,\n",
    "                              bw=bw)\n",
    "gwr = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the results\n",
    "gwr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a constant was added, so we'll add it to our predictors\n",
    "cols = ['constant'] + predictors\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn GWR local parameter estimates into a GeoDataFrame with tract geometries\n",
    "params = pd.DataFrame(gwr.params, columns=cols, index=X.index)\n",
    "params = tracts[['geometry']].merge(params, left_index=True, right_index=True, how='right')\n",
    "params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the spatial distribution of local parameter estimates\n",
    "# set nrows, ncols to match your number of predictors!\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "for col, ax in zip(predictors, axes.flat):\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(col)\n",
    "    gdf = params.dropna(subset=[col], axis='rows')\n",
    "    ax = gdf.plot(ax=ax,\n",
    "                  column=col,\n",
    "                  cmap=get_cmap(gdf[col]),\n",
    "                  legend=True,\n",
    "                  legend_kwds={'shrink': 0.6})\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn GWR local t-values into a GeoDataFrame with tract geometries\n",
    "# set t-values below significance threshold to zero then clip to ± 4\n",
    "tvals = pd.DataFrame(gwr.filter_tvals(alpha=0.05), columns=cols, index=X.index).clip(-4, 4)\n",
    "tvals = tracts[['geometry']].merge(tvals, left_index=True, right_index=True, how='right')\n",
    "\n",
    "# plot the spatial distribution of local t-values\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "for col, ax in zip(predictors, axes.flat):\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(col)\n",
    "    gdf = tvals.dropna(subset=[col], axis='rows')\n",
    "    ax = gdf.plot(ax=ax,\n",
    "                  column=col,\n",
    "                  cmap=get_cmap(gdf[col]),\n",
    "                  legend=True,\n",
    "                  legend_kwds={'shrink': 0.6})\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn GWR local R^2 values into a GeoDataFrame with tract geometries\n",
    "col = 'Local R-squared'\n",
    "r_squared = pd.DataFrame(gwr.localR2, index=X.index, columns=[col])\n",
    "r_squared = tracts[['geometry']].merge(r_squared, left_index=True, right_index=True, how='right')\n",
    "\n",
    "# plot the spatial distribution of local R^2\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title(col)\n",
    "gdf = r_squared.dropna(subset=[col], axis='rows')\n",
    "ax = gdf.plot(ax=ax,\n",
    "              column=col,\n",
    "              cmap=get_cmap(gdf[col]),\n",
    "              legend=True,\n",
    "              legend_kwds={'shrink': 0.6})\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spatial diagnostics\n",
    "\n",
    "So far we've seen different spatial heterogeneity models. Now we'll explore spatial dependence (modeling interdependencies between observations over space), starting by using queen-contiguity spatial weights to model spatial relationships between observations and OLS to check diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute spatial weights from tract geometries (but only those tracts that appear in design matrix!)\n",
    "W = ps.lib.weights.Queen.from_dataframe(tracts.loc[X.index])\n",
    "W.transform = 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute OLS spatial diagnostics to check the nature of spatial dependence\n",
    "ols = ps.model.spreg.OLS(y=Y.values,\n",
    "                         x=X.values,\n",
    "                         w=W,\n",
    "                         spat_diag=True,\n",
    "                         moran=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate moran's I (for the response) and its significance\n",
    "mi = ps.explore.esda.Moran(y=Y, w=W, two_tailed=True)\n",
    "print(mi.I)\n",
    "print(mi.p_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moran's I (for the residuals): moran's i, standardized i, p-value\n",
    "ols.moran_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the results\n",
    "\n",
    "A significant Moran's *I* suggests spatial autocorrelation, but doesn't tell us which alternative specification should be used. Lagrange Multiplier (LM) diagnostics can help with that. If one LM test is significant and the other isn't, then that tells us which model specification (spatial lag vs spatial error) to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagrange multiplier test for spatial lag model: stat, p\n",
    "ols.lm_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagrange multiplier test for spatial error model: stat, p\n",
    "ols.lm_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the results\n",
    "\n",
    "If (and only if) both the LM tests produce significant statistics, try the robust versions (the nonrobust LM tests are sensitive to each other):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robust lagrange multiplier test for spatial lag model: stat, p\n",
    "ols.rlm_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robust lagrange multiplier test for spatial error model: stat, p\n",
    "ols.rlm_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... which model specification to choose? Workflow:\n",
    "\n",
    "  1. If neither LM test is significant: use regular OLS.\n",
    "  2. If only one LM test is significant: use that model spec.\n",
    "  3. If both LM tests are significant: run robust versions.\n",
    "  4. If only one robust LM test is significant: use that model spec.\n",
    "  5. If both robust LM tests are significant (this can often happen with large sample sizes):\n",
    "     1. first consider if the initial model specification is actually a good fit\n",
    "     2. if so, use the spatial specification corresponding to the larger robust-LM statistic\n",
    "     3. or consider a combo model\n",
    "\n",
    "## 7. Spatial lag model\n",
    "\n",
    "When the diagnostics indicate the presence of a spatial diffusion process. Uses the spatially-lagged endogenous variable as a predictor. Because of endogeneity, cannot use OLS to estimate.\n",
    "\n",
    "Model specification:\n",
    "\n",
    "$y = \\rho W y + \\beta X + u$\n",
    "\n",
    "where $y$ is a $n \\times 1$ matrix of observations (response), $W$ is a $n \\times n$ spatial weights matrix (thus $Wy$ is the spatially-lagged response), $\\rho$ is the spatial autoregressive parameter to be estimated, $X$ is a $n \\times k$ matrix of observations (exogenous predictors), $\\beta$ is a $k \\times 1$ vector of parameters (coefficients) to be estimated, and $u$ is a $n \\times 1$ vector of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum-likelihood estimation with full matrix expression\n",
    "mll = ps.model.spreg.ML_Lag(y=Y.values,\n",
    "                            x=X.values,\n",
    "                            w=W,\n",
    "                            method='full',\n",
    "                            name_w='queen',\n",
    "                            name_x=X.columns.tolist(),\n",
    "                            name_y=response,\n",
    "                            name_ds='tracts')\n",
    "print(mll.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the spatial autoregressive parameter estimate, rho\n",
    "mll.rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Spatial error model\n",
    "\n",
    "When the diagnostics indicate the presence of spatial error dependence (spatial effects in error term).\n",
    "\n",
    "Model specification:\n",
    "\n",
    "$y = \\beta X + u$\n",
    "\n",
    "where $X$ is a $n \\times k$ matrix of observations (exogenous predictors), $\\beta$ is a $k \\times 1$ vector of parameters (coefficients) to be estimated, and $u$ is a $n \\times 1$ vector of errors. The errors $u$ follow a spatial autoregressive specification:\n",
    "\n",
    "$u = \\lambda Wu + \\epsilon$\n",
    "\n",
    "where $\\lambda$ is a spatial autoregressive parameter to be estimated and $\\epsilon$ is the vector of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum-likelihood estimation with full matrix expression\n",
    "mle = ps.model.spreg.ML_Error(y=Y.values,\n",
    "                              x=X.values,\n",
    "                              w=W,\n",
    "                              method='full',\n",
    "                              name_w='queen',\n",
    "                              name_x=X.columns.tolist(),\n",
    "                              name_y=response,\n",
    "                              name_ds='tracts')\n",
    "print(mle.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the spatial autoregressive parameter estimate, lambda\n",
    "mle.lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Spatial lag+error combo model\n",
    "\n",
    "Estimated with GMM (generalized method of moments). Essentially a spatial error model with endogenous explanatory variables.\n",
    "\n",
    "Model specification:\n",
    "\n",
    "$y = \\rho W y + \\beta X + u$\n",
    "\n",
    "where $y$ is a $n \\times 1$ matrix of observations (response), $W$ is a $n \\times n$ spatial weights matrix (thus $Wy$ is the spatially-lagged response), $\\rho$ is the spatial autoregressive parameter to be estimated, $X$ is a $n \\times k$ matrix of observations (exogenous predictors), $\\beta$ is a $k \\times 1$ vector of parameters (coefficients) to be estimated, and $u$ is a $n \\times 1$ vector of errors.\n",
    "\n",
    "The errors $u$ follow a spatial autoregressive specification:\n",
    "\n",
    "$u = \\lambda Wu + \\epsilon$\n",
    "\n",
    "where $\\lambda$ is a spatial autoregressive parameter to be estimated and $\\epsilon$ is the vector of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmc = ps.model.spreg.GM_Combo_Het(y=Y.values,\n",
    "                                  x=X.values,\n",
    "                                  w=W,\n",
    "                                  name_w='queen',\n",
    "                                  name_ds='tracts',\n",
    "                                  name_x=X.columns.tolist(),\n",
    "                                  name_y=response)\n",
    "print(gmc.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# with a new set of predictors, compute spatial diagnostics and estimate a new spatial model accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ppd599)",
   "language": "python",
   "name": "ppd599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
