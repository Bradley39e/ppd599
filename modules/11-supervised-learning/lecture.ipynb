{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "external-brooks",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "\n",
    "Machine learning's broad categories:\n",
    "  - supervised learning: train a model on observed (labeled) data to predict unobserved data\n",
    "    - classification: predict categorical variable\n",
    "    - regresssion: predict continuous variable\n",
    "  - unsupervised learning: discover structure in and extract information from unlabeled data\n",
    "    - clustering: assign observations to groups based on their features\n",
    "    - dimensionality reduction: transform a set of many features to a lower-dimension space (feature extraction)\n",
    "  - reinforcement learning: train model by rewarding it when it takes correct action\n",
    "  - artificial neural networks and deep learning\n",
    "    \n",
    "Tasks:\n",
    "  - data collection and cleaning\n",
    "  - feature selection: select a relevant subset of features to train your model\n",
    "  - feature extraction: apply a function to a feature to create a new feature\n",
    "  - model choice: [identify](https://scikit-learn.org/stable/tutorial/machine_learning_map/) the right kind of model/algorithm for the task\n",
    "  - model training: train the model on a set of training data\n",
    "  - model evaluation: assess its performance on a set of testing data, did it over or underfit?\n",
    "  - hyperparameter tuning: adjust the hyperparameters the algorithm uses to fit the model for optimum performance\n",
    "  - prediction: use the model to make predictions on unseen data\n",
    "\n",
    "### Probability refresher\n",
    "\n",
    "**Probability** is the ratio of an event occuring to all possible events occurring, whereas the **odds** are the ratio of an event occuring to it not occurring. That is, the odds are the ratio of the probability of an event occurring to the probability of it not occurring: $\\text{odds}=\\frac{p}{1-p}$ and conversely $p=\\frac{\\text{odds}}{1 + \\text{odds}}$\n",
    "\n",
    "For example, if there are 8 blue marbles and 2 red marbles in an urn, the probability of drawing a blue marble is $\\frac{8}{8+2}=0.8$, its odds are $\\frac{8}{2}=4$ (often expressed $4:1$) which is equivalent to $\\frac{0.8}{1-0.8}=4$, and its log-odds are therefore $\\log(4)=1.386$.\n",
    "\n",
    "**Log-odds** (the logarithm of the odds) are useful because they take odds asymmetrically distributed around 1 and transform them symmetrically around 0, such that $\\log(4)=-\\log(\\frac{1}{4})$, and allow us to linearly combine **odds ratios** by simply adding and subtracting (because the log of a ratio is the log of the numerator minus the log of the denominator). An odds ratio is just the ratio of two odds, useful when comparing the odds of a \"what if\" scenario to the odds of the base scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CA tract-level census variables\n",
    "df = pd.read_csv('../../data/census_tracts_data_ca.csv', dtype={'GEOID10':str}).set_index('GEOID10')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-beatles",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "Logistic regression is a regression analysis technique used when the response is binary. It uses maximum likelihood estimation (with regularization) to estimate the parameters of a logit model. The logit model of some probability $p$ represents its log-odds:\n",
    "\n",
    "$\\text{logit}(p) = \\log{\\frac{p}{1-p}} = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_k X_k$\n",
    "\n",
    "In our example, $p$ represents the probability of being assigned to one of the classes in our classification scheme. The logit is the inverse of the logistic function.\n",
    "\n",
    "**We will build some simple models to predict tract poverty status.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify tracts into high poverty vs not\n",
    "df['poverty'] = (df['pct_below_poverty'] > 20).astype(int)\n",
    "df['poverty'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "response = 'poverty'\n",
    "predictors = ['median_age', 'pct_renting', 'pct_bachelors_degree', 'pct_english_only']\n",
    "data = df[[response] + predictors].dropna()\n",
    "y = data[response]\n",
    "X = data[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "lr = LogisticRegression()\n",
    "y_pred = lr.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the probabilities\n",
    "probs = lr.predict_proba(X_test)\n",
    "df_probs = pd.DataFrame(probs, columns=lr.classes_)\n",
    "df_probs['pred'] = y_pred\n",
    "df_probs['actual'] = y_test.values\n",
    "df_probs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-durham",
   "metadata": {},
   "source": [
    "Manually calculate the probability of observation $i$ belonging to class $1$ as $p = \\frac{e^{\\beta X_i}}{1 + e^{\\beta X_i}}$ where $\\text{logit}(p) = \\beta X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the logit (log-odds) for observation 0 and convert to probability\n",
    "# this is its probability of being assigned to class 1\n",
    "log_odds = np.dot(lr.coef_, X_test[0]) + lr.intercept_\n",
    "odds = np.exp(log_odds)\n",
    "prob = odds / (1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-protein",
   "metadata": {},
   "source": [
    "## 2. Classification into multiple categories\n",
    "\n",
    "Binomial logistic regression's predictions assign observations to one of two classes, but many real-world scenarios require three or more classes. The rest of today's examples will use a multi-class scheme by categorizing tracts into \"low\", \"mid\", or \"high\" poverty status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a poverty classification variable\n",
    "# by default, set all as mid poverty tracts\n",
    "df['poverty'] = 'mid'\n",
    "\n",
    "# identify all low poverty tracts\n",
    "mask_low = df['pct_below_poverty'] <= 5\n",
    "df.loc[mask_low, 'poverty'] = 'low'\n",
    "\n",
    "# identify all high poverty tracts\n",
    "mask_high = df['pct_below_poverty'] >= 25\n",
    "df.loc[mask_high, 'poverty'] = 'high'\n",
    "\n",
    "df['poverty'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "response = 'poverty'\n",
    "predictors = ['median_age', 'pct_renting', 'pct_bachelors_degree', 'pct_english_only']\n",
    "data = df[[response] + predictors].dropna()\n",
    "y = data[response]\n",
    "X = data[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-divide",
   "metadata": {},
   "source": [
    "## 3. Multinomial Logistic Regression\n",
    "\n",
    "Multinomial logistic regression is a generalization of binomial logistic regression to multiple classes. That is, it is a regression analysis technique used when the response is categorical and contains >2 classes. Multinomial logistic regression uses the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function to generalize the logistic function to multiple inputs: in probability theory, the softmax represents a probability distribution across a set of $C$ possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "lr = LogisticRegression(multi_class='multinomial')\n",
    "y_pred = lr.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-column",
   "metadata": {},
   "source": [
    "### Making sense of the probabilities\n",
    "\n",
    "Let's inspect the estimated probabilities of observation $i$ belonging to class $c$ given $\\beta$ and $X_i$, the estimated coefficents and $i$'s features.\n",
    "\n",
    "Then, manually calculate the logit, normalize via softmax, and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = lr.predict_proba(X_test)\n",
    "df_probs = pd.DataFrame(probs, columns=lr.classes_)\n",
    "df_probs['pred'] = y_pred\n",
    "df_probs['actual'] = y_test.values\n",
    "df_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick an observation\n",
    "i = 0\n",
    "\n",
    "# calculate the logit (log-odds) then normalize with softmax function\n",
    "log_odds = np.dot(lr.coef_, X_test[i]) + lr.intercept_\n",
    "prob = np.exp(log_odds) / np.exp(log_odds).sum()\n",
    "\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-collective",
   "metadata": {},
   "source": [
    "### Making sense of the coefficients\n",
    "\n",
    "Logistic regression is a parametric method. We have estimated the parameters (coefficients) of a logit model. scikit-learn treats logistic regression in the predictive ML paradigm: for a traditional statistical inference treatment of logistic regression, use the statsmodels package instead.\n",
    "\n",
    "Each estimated coefficient is the log of the odds ratio. An **odds ratio** is the ratio of the odds of the event occurring to the odds of it not occurring. In our case the event is a 1-unit increase in the predictor. Thus the logit coefficient $\\beta_{c,k}=\\log\\frac{\\text{odds}(c | X_k+1)}{\\text{odds}(c | X_k)}$, that is, the ceteris paribus log of the odds of an observation being in class $c$ if $x_k$ is incremented by $1$, divided by the odds of it being in class $c$ if nothing changes. Conversely, the odds ratio is the exponentiated logit coefficient: $\\text{odds ratio} = \\frac{\\text{odds}(c | X_k+1)}{\\text{odds}(c | X_k)} = e^{\\beta_{c,k}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimated coefficients on each variable, for each class\n",
    "df_coeffs = pd.DataFrame(lr.coef_, columns=X.columns, index=lr.classes_)\n",
    "df_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the odds ratio for some class and some predictor\n",
    "# a 1-unit increase in predictor k increases the odds of class c by what %\n",
    "B_ck = df_coeffs.loc['low', 'pct_english_only']\n",
    "odds_ratio = np.exp(B_ck)\n",
    "odds_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-duration",
   "metadata": {},
   "source": [
    "Given an odds ratio $R$, the percent change $\\delta$ in the odds can be calculated as $\\delta = 100 \\times (R - 1)$\n",
    "\n",
    "That is, a 1-unit increase in the percent that speak English-only at home is associated with a $\\delta$% increase in the odds of being classified in the low poverty category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually calculate the odds ratio for some observation, class, and predictor\n",
    "i = 0  # observation in position 0 (you can pick any one)\n",
    "c = 1  # class in position 1 (ie, \"low\")\n",
    "k = 3  # predictor in position 3 (ie, \"pct_english_only\")\n",
    "\n",
    "# calculate the logit of class c if nothing changes, then convert to odds\n",
    "x0 = X_test[i]\n",
    "log_odds0 = np.dot(lr.coef_, X_test[i]) + lr.intercept_\n",
    "odds0 = np.exp(log_odds0[c]) # convert log-odds to odds\n",
    "\n",
    "# calculate the logit of class c if we increase k by 1, then convert to odds\n",
    "x1 = x0.copy()\n",
    "x1[k] = x1[k] + 1\n",
    "log_odds1 = np.dot(lr.coef_, x1) + lr.intercept_\n",
    "odds1 = np.exp(log_odds1[c]) # convert log-odds to odds\n",
    "\n",
    "# calculate the odds ratio\n",
    "odds_ratio = odds1 / odds0\n",
    "odds_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-stevens",
   "metadata": {},
   "source": [
    "## 4. Model Assessment\n",
    "\n",
    "The model's performance can be assessed via several validation methods, including:\n",
    "\n",
    "  - holdout method: fit model to one subset of data, then test it on a different subset\n",
    "  - *k*-fold cross validation: divide data into *k* groups then, for each group, train the model on all the *other* groups, then test the model on the group and record its assessment score\n",
    "  - bootstrapping: sample with replacement from dataset to assess accuracy\n",
    "  \n",
    "Typical assessments to report include:\n",
    "  - precision: what share of all true + false positives are true positives? That is, among everything predicted to be in this class, how many were right?\n",
    "  - recall, aka sensitivity = true positive rate: what share of all true positives + false negatives are true positives? That is, among all the actual items in this class, how many were predicted correctly?\n",
    "  - specificity = true negative rate: what share of all true negatives + false positives are true negatives?\n",
    "  - $F_1$ score: an overall measure of accuracy: the harmonic mean of precision and recall\n",
    "  - plot ROC curves: true positives vs false positives, and measure area under curve\n",
    "  \n",
    "Bias-variance tradeoff: you want a model that both 1) captures the nuanced patterns of the training data and 2) generalizes well to new data. However you cannot improve both at the same time, you must trade them off. Overfitting means high variance: your model is too sensitive to noise in the training data and may need regularization. Underfitting means high bias: your model is too smooth and misses important details in the training data.\n",
    "\n",
    "Next steps: assess your model, then tune its hyperparameters to improve its performance, then re-assess. Judicious feature selection, dimensionality reduction, and larger training sample sizes can reduce variance (overfitting). Adding additional predictors can reduce bias (underfitting). Consider using a hyperparameter optimization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "# the report tells about the quality of its predictions\n",
    "# support means how many of each class it saw\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-advocate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to visualize the model's decision surface\n",
    "# fits model pairwise to just 2 features at a time and plots them\n",
    "def plot_decision(X, y, feature_names, classifier):\n",
    "    \n",
    "    class_colors = {'high': 'r', 'mid': 'y', 'low': 'b'}\n",
    "    class_ints = {'high': 0, 'mid': 1, 'low': 2}\n",
    "    plot_colors = ['r', 'y', 'b']\n",
    "    plot_step = 0.02\n",
    "    pairs = [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "    for ax, pair in zip(axes.flat, pairs):\n",
    "        \n",
    "        # take the two corresponding features\n",
    "        Xp = X[:, pair]\n",
    "        x_min, x_max = Xp[:, 0].min() - 1, Xp[:, 0].max() + 1\n",
    "        y_min, y_max = Xp[:, 1].min() - 1, Xp[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                             np.arange(y_min, y_max, plot_step))\n",
    "        \n",
    "        Z = classifier.fit(Xp, y).predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "        for cat, i in class_ints.items():\n",
    "            Z[np.where(Z==cat)] = i\n",
    "        cs = ax.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "\n",
    "        for cat, color in class_colors.items():\n",
    "            #print(cat, color)\n",
    "            idx = np.where(y == cat)\n",
    "            ax.scatter(Xp[idx, 0], Xp[idx, 1], c=color, label=cat, s=1)\n",
    "        \n",
    "        #ax.set_xlim(x_min, x_max)\n",
    "        #ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xlabel(feature_names[pair[0]])\n",
    "        ax.set_ylabel(feature_names[pair[1]])\n",
    "        ax.figure.tight_layout()\n",
    "        \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface\n",
    "# fits model pairwise to just 2 features at a time and plots them\n",
    "plot_decision(X_train, y_train, X.columns, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-brief",
   "metadata": {},
   "source": [
    "## 5. Decision Trees and Random Forests\n",
    "\n",
    "The logistic regression models we saw earlier were parametric models. A decision tree is a nonparametric model. It has a tendency to overfit data, but ensembles can help prevent it from getting stuck in a local minimum. Ensemble techniques can be broadly divided into averaging methods and boosting methods.\n",
    "\n",
    "A random forest is a nonparametric model and an ensemble learning method that constructs multiple decision trees (then takes the mode). This helps correct for decision trees' overfitting the training data. Random forests tend to work well out of the box, handle nonlinearity well, and work well in very high dimension spaces.\n",
    "\n",
    "Note we didn't have to standardize the data to train a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "# set max_depth\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "y_pred = dt.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface\n",
    "plot_decision(X_train, y_train, X.columns, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# try setting max_depth to None. what happens? why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train model on training data then use it to make predictions with test data\n",
    "# use 1,000 decision trees and all available CPUs\n",
    "rf = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n",
    "y_pred = rf.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface\n",
    "plot_decision(X_train, y_train, X.columns, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-bench",
   "metadata": {},
   "source": [
    "## 6. *k*-Nearest Neighbors\n",
    "\n",
    "kNN is a nonlinear, nonparametric, lazy-learning model and represents an example of instance-based learning. It can require a lot of memory but works well with a small number of dimensions, though possibly less well with high dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "y_pred = knn.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface\n",
    "plot_decision(X_train, y_train, X.columns, knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-expansion",
   "metadata": {},
   "source": [
    "## 7. Naïve Bayes\n",
    "\n",
    "Naïve Bayes is a high bias/low variance classifier that is less likely to overfit small training datasets than a low bias/high variance classifier is (such as kNN or logistic regression). It is a simple algorithm and converges quickly but strongly assumes independence (hence, naïve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "gnb = GaussianNB(priors=None)\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface\n",
    "plot_decision(X_train, y_train, X.columns, gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-paraguay",
   "metadata": {},
   "source": [
    "## 8. Perceptron\n",
    "\n",
    "A perceptron is a simple supervised learning linear binary classifier. It is a very simple single-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "ppn = Perceptron(eta0=1)\n",
    "y_pred = ppn.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface\n",
    "plot_decision(X_train, y_train, X.columns, ppn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-disease",
   "metadata": {},
   "source": [
    "## 9. Support Vector Machines\n",
    "\n",
    "SVMs are nonparametric models that find an optimum hyperplane that represents the largest margin (that is, separation) between the classes of training data (the training data is aka support vectors). In other words, SVM finds the hyperplane that maximizes the distance between it and the nearest data point on either side of it. SVMs can classify data linearly or, using the kernel trick, nonlinearly (if the classes are not separable linearly).\n",
    "\n",
    "An SVM with a linear kernel is very similar to logistic regression. But they might be a good choice instead of logistic regression if the problem is not linearly separable. SVMs also work well in high-dimensional space.\n",
    "\n",
    "Choosing the right kernel can be challenging. The results are not straightforwardly explainable. SVMs can be very inefficient to train, so not a good choice for large training data sets.\n",
    "\n",
    "Tuning the SVM's hyperparameters is critical! Here we fit an untuned model as a quick demo, but you should run something like [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "# train the linear SVM (namely, support vector classification)\n",
    "svc = SVC(kernel='linear', C=1)\n",
    "y_pred = svc.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-prototype",
   "metadata": {},
   "source": [
    "### SVMs for nonlinear classification with a kernel function\n",
    "\n",
    "We can turn a linear SVM model into a nonlinear model by using the \"kernel trick\" to operate in a higher-dimension feature space. We will use the radial basis function kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "svc_kt = SVC(kernel='rbf', gamma=0.2, C=1)\n",
    "y_pred = svc_kt.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface: this is slow\n",
    "plot_decision(X_train, y_train, X.columns, svc_kt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-setting",
   "metadata": {},
   "source": [
    "Higher gamma parameter values lead to tighter decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on training data then use it to make predictions with test data\n",
    "svc_kt2 = SVC(kernel='rbf', gamma=10, C=1)\n",
    "y_pred = svc_kt2.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did the classifier perform?\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model's decision surface: this is slow\n",
    "plot_decision(X_train, y_train, X.columns, svc_kt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-savannah",
   "metadata": {},
   "source": [
    "Here we see poor generalization because the model was overfitted with that high gamma value: the training overemphasized small fluctations in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-sussex",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ppd599)",
   "language": "python",
   "name": "ppd599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
